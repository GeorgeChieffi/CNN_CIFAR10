{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GeorgeChieffi/CNN_CIFAR10/blob/main/Proj1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1Zkknzn9UsJx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dd4bca1-03f0-474d-cdee-17da78b2d6dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./content/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:02<00:00, 61106519.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./content/cifar-10-python.tar.gz to ./content/\n",
            "Files already downloaded and verified\n",
            "Running on:  cuda\n",
            "-----\n",
            "Training: Epoch 1/100 0.01619 Loss, 41.84% Accurate\n",
            "Validation: Epoch 1/100 0.01340 Loss, 51.40% Accurate\n",
            "-----\n",
            "Training: Epoch 2/100 0.01260 Loss, 54.62% Accurate\n",
            "Validation: Epoch 2/100 0.01165 Loss, 58.14% Accurate\n",
            "-----\n",
            "Training: Epoch 3/100 0.01111 Loss, 60.21% Accurate\n",
            "Validation: Epoch 3/100 0.01065 Loss, 61.70% Accurate\n",
            "-----\n",
            "Training: Epoch 4/100 0.01014 Loss, 63.92% Accurate\n",
            "Validation: Epoch 4/100 0.00996 Loss, 64.00% Accurate\n",
            "-----\n",
            "Training: Epoch 5/100 0.00943 Loss, 66.46% Accurate\n",
            "Validation: Epoch 5/100 0.00941 Loss, 66.56% Accurate\n",
            "-----\n",
            "Training: Epoch 6/100 0.00885 Loss, 68.59% Accurate\n",
            "Validation: Epoch 6/100 0.00915 Loss, 67.54% Accurate\n",
            "-----\n",
            "Training: Epoch 7/100 0.00838 Loss, 70.31% Accurate\n",
            "Validation: Epoch 7/100 0.00883 Loss, 68.62% Accurate\n",
            "-----\n",
            "Training: Epoch 8/100 0.00796 Loss, 71.83% Accurate\n",
            "Validation: Epoch 8/100 0.00856 Loss, 69.78% Accurate\n",
            "-----\n",
            "Training: Epoch 9/100 0.00761 Loss, 73.25% Accurate\n",
            "Validation: Epoch 9/100 0.00840 Loss, 70.30% Accurate\n",
            "-----\n",
            "Training: Epoch 10/100 0.00728 Loss, 74.42% Accurate\n",
            "Validation: Epoch 10/100 0.00826 Loss, 71.00% Accurate\n",
            "-----\n",
            "Training: Epoch 11/100 0.00699 Loss, 75.42% Accurate\n",
            "Validation: Epoch 11/100 0.00816 Loss, 71.02% Accurate\n",
            "-----\n",
            "Training: Epoch 12/100 0.00671 Loss, 76.48% Accurate\n",
            "Validation: Epoch 12/100 0.00810 Loss, 71.18% Accurate\n",
            "-----\n",
            "Training: Epoch 13/100 0.00646 Loss, 77.50% Accurate\n",
            "Validation: Epoch 13/100 0.00801 Loss, 71.70% Accurate\n",
            "-----\n",
            "Training: Epoch 14/100 0.00622 Loss, 78.39% Accurate\n",
            "Validation: Epoch 14/100 0.00796 Loss, 72.18% Accurate\n",
            "-----\n",
            "Training: Epoch 15/100 0.00600 Loss, 79.21% Accurate\n",
            "Validation: Epoch 15/100 0.00791 Loss, 71.96% Accurate\n",
            "-----\n",
            "Training: Epoch 16/100 0.00578 Loss, 80.01% Accurate\n",
            "Validation: Epoch 16/100 0.00780 Loss, 72.68% Accurate\n",
            "-----\n",
            "Training: Epoch 17/100 0.00557 Loss, 80.82% Accurate\n",
            "Validation: Epoch 17/100 0.00775 Loss, 72.94% Accurate\n",
            "-----\n",
            "Training: Epoch 18/100 0.00536 Loss, 81.64% Accurate\n",
            "Validation: Epoch 18/100 0.00770 Loss, 73.52% Accurate\n",
            "-----\n",
            "Training: Epoch 19/100 0.00516 Loss, 82.45% Accurate\n",
            "Validation: Epoch 19/100 0.00763 Loss, 73.62% Accurate\n",
            "-----\n",
            "Training: Epoch 20/100 0.00496 Loss, 83.10% Accurate\n",
            "Validation: Epoch 20/100 0.00761 Loss, 73.90% Accurate\n",
            "-----\n",
            "Training: Epoch 21/100 0.00476 Loss, 83.95% Accurate\n",
            "Validation: Epoch 21/100 0.00760 Loss, 73.96% Accurate\n",
            "-----\n",
            "Training: Epoch 22/100 0.00457 Loss, 84.65% Accurate\n",
            "Validation: Epoch 22/100 0.00760 Loss, 74.38% Accurate\n",
            "-----\n",
            "Training: Epoch 23/100 0.00437 Loss, 85.46% Accurate\n",
            "Validation: Epoch 23/100 0.00762 Loss, 74.52% Accurate\n",
            "-----\n",
            "Training: Epoch 24/100 0.00418 Loss, 86.26% Accurate\n",
            "Validation: Epoch 24/100 0.00767 Loss, 74.60% Accurate\n",
            "-----\n",
            "Training: Epoch 25/100 0.00399 Loss, 87.07% Accurate\n",
            "Validation: Epoch 25/100 0.00770 Loss, 74.72% Accurate\n",
            "-----\n",
            "Training: Epoch 26/100 0.00380 Loss, 87.78% Accurate\n",
            "Validation: Epoch 26/100 0.00776 Loss, 74.74% Accurate\n",
            "-----\n",
            "Training: Epoch 27/100 0.00361 Loss, 88.56% Accurate\n",
            "Validation: Epoch 27/100 0.00780 Loss, 74.60% Accurate\n",
            "-----\n",
            "Training: Epoch 28/100 0.00343 Loss, 89.29% Accurate\n",
            "Validation: Epoch 28/100 0.00786 Loss, 74.80% Accurate\n",
            "-----\n",
            "Training: Epoch 29/100 0.00324 Loss, 90.18% Accurate\n",
            "Validation: Epoch 29/100 0.00792 Loss, 74.80% Accurate\n",
            "-----\n",
            "Training: Epoch 30/100 0.00306 Loss, 90.82% Accurate\n",
            "Validation: Epoch 30/100 0.00799 Loss, 74.96% Accurate\n",
            "-----\n",
            "Training: Epoch 31/100 0.00288 Loss, 91.56% Accurate\n",
            "Validation: Epoch 31/100 0.00808 Loss, 74.92% Accurate\n",
            "-----\n",
            "Training: Epoch 32/100 0.00270 Loss, 92.37% Accurate\n",
            "Validation: Epoch 32/100 0.00816 Loss, 74.56% Accurate\n",
            "-----\n",
            "Training: Epoch 33/100 0.00253 Loss, 93.21% Accurate\n",
            "Validation: Epoch 33/100 0.00823 Loss, 74.66% Accurate\n",
            "-----\n",
            "Training: Epoch 34/100 0.00236 Loss, 93.91% Accurate\n",
            "Validation: Epoch 34/100 0.00834 Loss, 74.62% Accurate\n",
            "-----\n",
            "Training: Epoch 35/100 0.00219 Loss, 94.53% Accurate\n",
            "Validation: Epoch 35/100 0.00846 Loss, 74.64% Accurate\n",
            "-----\n",
            "Training: Epoch 36/100 0.00203 Loss, 95.25% Accurate\n",
            "Validation: Epoch 36/100 0.00858 Loss, 74.74% Accurate\n",
            "-----\n",
            "Training: Epoch 37/100 0.00187 Loss, 95.83% Accurate\n",
            "Validation: Epoch 37/100 0.00871 Loss, 74.70% Accurate\n",
            "-----\n",
            "Training: Epoch 38/100 0.00172 Loss, 96.44% Accurate\n",
            "Validation: Epoch 38/100 0.00883 Loss, 74.86% Accurate\n",
            "-----\n",
            "Training: Epoch 39/100 0.00157 Loss, 96.95% Accurate\n",
            "Validation: Epoch 39/100 0.00896 Loss, 74.78% Accurate\n",
            "-----\n",
            "Training: Epoch 40/100 0.00144 Loss, 97.42% Accurate\n",
            "Validation: Epoch 40/100 0.00906 Loss, 74.78% Accurate\n",
            "-----\n",
            "Training: Epoch 41/100 0.00131 Loss, 97.89% Accurate\n",
            "Validation: Epoch 41/100 0.00919 Loss, 74.98% Accurate\n",
            "-----\n",
            "Training: Epoch 42/100 0.00119 Loss, 98.27% Accurate\n",
            "Validation: Epoch 42/100 0.00932 Loss, 74.88% Accurate\n",
            "-----\n",
            "Training: Epoch 43/100 0.00108 Loss, 98.64% Accurate\n",
            "Validation: Epoch 43/100 0.00943 Loss, 74.90% Accurate\n",
            "-----\n",
            "Training: Epoch 44/100 0.00098 Loss, 98.93% Accurate\n",
            "Validation: Epoch 44/100 0.00956 Loss, 74.92% Accurate\n",
            "-----\n",
            "Training: Epoch 45/100 0.00089 Loss, 99.16% Accurate\n",
            "Validation: Epoch 45/100 0.00966 Loss, 75.14% Accurate\n",
            "-----\n",
            "Training: Epoch 46/100 0.00080 Loss, 99.37% Accurate\n",
            "Validation: Epoch 46/100 0.00982 Loss, 74.92% Accurate\n",
            "-----\n",
            "Training: Epoch 47/100 0.00073 Loss, 99.52% Accurate\n",
            "Validation: Epoch 47/100 0.00997 Loss, 74.92% Accurate\n",
            "-----\n",
            "Training: Epoch 48/100 0.00066 Loss, 99.64% Accurate\n",
            "Validation: Epoch 48/100 0.01009 Loss, 74.96% Accurate\n",
            "-----\n",
            "Training: Epoch 49/100 0.00060 Loss, 99.73% Accurate\n",
            "Validation: Epoch 49/100 0.01023 Loss, 74.86% Accurate\n",
            "-----\n",
            "Training: Epoch 50/100 0.00055 Loss, 99.79% Accurate\n",
            "Validation: Epoch 50/100 0.01035 Loss, 74.98% Accurate\n",
            "-----\n",
            "Training: Epoch 51/100 0.00050 Loss, 99.83% Accurate\n",
            "Validation: Epoch 51/100 0.01046 Loss, 74.98% Accurate\n",
            "-----\n",
            "Training: Epoch 52/100 0.00046 Loss, 99.88% Accurate\n",
            "Validation: Epoch 52/100 0.01056 Loss, 74.86% Accurate\n",
            "-----\n",
            "Training: Epoch 53/100 0.00042 Loss, 99.90% Accurate\n",
            "Validation: Epoch 53/100 0.01068 Loss, 74.72% Accurate\n",
            "-----\n",
            "Training: Epoch 54/100 0.00039 Loss, 99.93% Accurate\n",
            "Validation: Epoch 54/100 0.01079 Loss, 74.72% Accurate\n",
            "-----\n",
            "Training: Epoch 55/100 0.00036 Loss, 99.96% Accurate\n",
            "Validation: Epoch 55/100 0.01090 Loss, 74.74% Accurate\n",
            "-----\n",
            "Training: Epoch 56/100 0.00034 Loss, 99.96% Accurate\n",
            "Validation: Epoch 56/100 0.01099 Loss, 74.72% Accurate\n",
            "-----\n",
            "Training: Epoch 57/100 0.00031 Loss, 99.97% Accurate\n",
            "Validation: Epoch 57/100 0.01107 Loss, 74.80% Accurate\n",
            "-----\n",
            "Training: Epoch 58/100 0.00029 Loss, 99.97% Accurate\n",
            "Validation: Epoch 58/100 0.01115 Loss, 74.88% Accurate\n",
            "-----\n",
            "Training: Epoch 59/100 0.00027 Loss, 99.98% Accurate\n",
            "Validation: Epoch 59/100 0.01123 Loss, 74.94% Accurate\n",
            "-----\n",
            "Training: Epoch 60/100 0.00025 Loss, 99.98% Accurate\n",
            "Validation: Epoch 60/100 0.01130 Loss, 74.94% Accurate\n",
            "-----\n",
            "Training: Epoch 61/100 0.00024 Loss, 99.99% Accurate\n",
            "Validation: Epoch 61/100 0.01137 Loss, 74.84% Accurate\n",
            "-----\n",
            "Training: Epoch 62/100 0.00022 Loss, 99.99% Accurate\n",
            "Validation: Epoch 62/100 0.01143 Loss, 74.82% Accurate\n",
            "-----\n",
            "Training: Epoch 63/100 0.00021 Loss, 99.99% Accurate\n",
            "Validation: Epoch 63/100 0.01149 Loss, 74.78% Accurate\n",
            "-----\n",
            "Training: Epoch 64/100 0.00020 Loss, 100.00% Accurate\n",
            "Validation: Epoch 64/100 0.01155 Loss, 74.80% Accurate\n",
            "-----\n",
            "Training: Epoch 65/100 0.00019 Loss, 100.00% Accurate\n",
            "Validation: Epoch 65/100 0.01161 Loss, 74.74% Accurate\n",
            "-----\n",
            "Training: Epoch 66/100 0.00018 Loss, 100.00% Accurate\n",
            "Validation: Epoch 66/100 0.01167 Loss, 74.72% Accurate\n",
            "-----\n",
            "Training: Epoch 67/100 0.00017 Loss, 100.00% Accurate\n",
            "Validation: Epoch 67/100 0.01172 Loss, 74.84% Accurate\n",
            "-----\n",
            "Training: Epoch 68/100 0.00016 Loss, 100.00% Accurate\n",
            "Validation: Epoch 68/100 0.01177 Loss, 74.92% Accurate\n",
            "-----\n",
            "Training: Epoch 69/100 0.00015 Loss, 100.00% Accurate\n",
            "Validation: Epoch 69/100 0.01183 Loss, 75.00% Accurate\n",
            "-----\n",
            "Training: Epoch 70/100 0.00015 Loss, 100.00% Accurate\n",
            "Validation: Epoch 70/100 0.01188 Loss, 74.86% Accurate\n",
            "-----\n",
            "Training: Epoch 71/100 0.00014 Loss, 100.00% Accurate\n",
            "Validation: Epoch 71/100 0.01194 Loss, 74.88% Accurate\n",
            "-----\n",
            "Training: Epoch 72/100 0.00013 Loss, 100.00% Accurate\n",
            "Validation: Epoch 72/100 0.01199 Loss, 75.00% Accurate\n",
            "-----\n",
            "Training: Epoch 73/100 0.00013 Loss, 100.00% Accurate\n",
            "Validation: Epoch 73/100 0.01204 Loss, 75.02% Accurate\n",
            "-----\n",
            "Training: Epoch 74/100 0.00012 Loss, 100.00% Accurate\n",
            "Validation: Epoch 74/100 0.01209 Loss, 74.98% Accurate\n",
            "-----\n",
            "Training: Epoch 75/100 0.00012 Loss, 100.00% Accurate\n",
            "Validation: Epoch 75/100 0.01214 Loss, 74.96% Accurate\n",
            "-----\n",
            "Training: Epoch 76/100 0.00011 Loss, 100.00% Accurate\n",
            "Validation: Epoch 76/100 0.01218 Loss, 75.02% Accurate\n",
            "-----\n",
            "Training: Epoch 77/100 0.00011 Loss, 100.00% Accurate\n",
            "Validation: Epoch 77/100 0.01223 Loss, 75.14% Accurate\n",
            "-----\n",
            "Training: Epoch 78/100 0.00010 Loss, 100.00% Accurate\n",
            "Validation: Epoch 78/100 0.01228 Loss, 75.14% Accurate\n",
            "-----\n",
            "Training: Epoch 79/100 0.00010 Loss, 100.00% Accurate\n",
            "Validation: Epoch 79/100 0.01232 Loss, 75.26% Accurate\n",
            "-----\n",
            "Training: Epoch 80/100 0.00010 Loss, 100.00% Accurate\n",
            "Validation: Epoch 80/100 0.01237 Loss, 75.32% Accurate\n",
            "-----\n",
            "Training: Epoch 81/100 0.00009 Loss, 100.00% Accurate\n",
            "Validation: Epoch 81/100 0.01242 Loss, 75.36% Accurate\n",
            "-----\n",
            "Training: Epoch 82/100 0.00009 Loss, 100.00% Accurate\n",
            "Validation: Epoch 82/100 0.01246 Loss, 75.36% Accurate\n",
            "-----\n",
            "Training: Epoch 83/100 0.00009 Loss, 100.00% Accurate\n",
            "Validation: Epoch 83/100 0.01250 Loss, 75.42% Accurate\n",
            "-----\n",
            "Training: Epoch 84/100 0.00008 Loss, 100.00% Accurate\n",
            "Validation: Epoch 84/100 0.01254 Loss, 75.50% Accurate\n",
            "-----\n",
            "Training: Epoch 85/100 0.00008 Loss, 100.00% Accurate\n",
            "Validation: Epoch 85/100 0.01259 Loss, 75.58% Accurate\n",
            "-----\n",
            "Training: Epoch 86/100 0.00008 Loss, 100.00% Accurate\n",
            "Validation: Epoch 86/100 0.01263 Loss, 75.56% Accurate\n",
            "-----\n",
            "Training: Epoch 87/100 0.00008 Loss, 100.00% Accurate\n",
            "Validation: Epoch 87/100 0.01267 Loss, 75.56% Accurate\n",
            "-----\n",
            "Training: Epoch 88/100 0.00007 Loss, 100.00% Accurate\n",
            "Validation: Epoch 88/100 0.01271 Loss, 75.56% Accurate\n",
            "-----\n",
            "Training: Epoch 89/100 0.00007 Loss, 100.00% Accurate\n",
            "Validation: Epoch 89/100 0.01275 Loss, 75.64% Accurate\n",
            "-----\n",
            "Training: Epoch 90/100 0.00007 Loss, 100.00% Accurate\n",
            "Validation: Epoch 90/100 0.01278 Loss, 75.64% Accurate\n",
            "-----\n",
            "Training: Epoch 91/100 0.00007 Loss, 100.00% Accurate\n",
            "Validation: Epoch 91/100 0.01282 Loss, 75.62% Accurate\n",
            "-----\n",
            "Training: Epoch 92/100 0.00007 Loss, 100.00% Accurate\n",
            "Validation: Epoch 92/100 0.01286 Loss, 75.66% Accurate\n",
            "-----\n",
            "Training: Epoch 93/100 0.00006 Loss, 100.00% Accurate\n",
            "Validation: Epoch 93/100 0.01290 Loss, 75.66% Accurate\n",
            "-----\n",
            "Training: Epoch 94/100 0.00006 Loss, 100.00% Accurate\n",
            "Validation: Epoch 94/100 0.01293 Loss, 75.66% Accurate\n",
            "-----\n",
            "Training: Epoch 95/100 0.00006 Loss, 100.00% Accurate\n",
            "Validation: Epoch 95/100 0.01297 Loss, 75.66% Accurate\n",
            "-----\n",
            "Training: Epoch 96/100 0.00006 Loss, 100.00% Accurate\n",
            "Validation: Epoch 96/100 0.01300 Loss, 75.72% Accurate\n",
            "-----\n",
            "Training: Epoch 97/100 0.00006 Loss, 100.00% Accurate\n",
            "Validation: Epoch 97/100 0.01304 Loss, 75.82% Accurate\n",
            "-----\n",
            "Training: Epoch 98/100 0.00006 Loss, 100.00% Accurate\n",
            "Validation: Epoch 98/100 0.01307 Loss, 75.78% Accurate\n",
            "-----\n",
            "Training: Epoch 99/100 0.00006 Loss, 100.00% Accurate\n",
            "Validation: Epoch 99/100 0.01311 Loss, 75.84% Accurate\n",
            "-----\n",
            "Training: Epoch 100/100 0.00005 Loss, 100.00% Accurate\n",
            "Validation: Epoch 100/100 0.01314 Loss, 75.80% Accurate\n",
            "-----\n",
            "Test: Epoch 100/100 Training: 0.01413 Loss, 74.73% Accurate\n"
          ]
        }
      ],
      "source": [
        "import torchvision\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "\n",
        "# Create an 'output' directory if not already exists\n",
        "outdir = 'output'\n",
        "os.makedirs(outdir, exist_ok=True)\n",
        "\n",
        "# Load CIFAR-10 training set\n",
        "data_train = torchvision.datasets.CIFAR10('./content/', download=True, train=True)\n",
        "data_test = torchvision.datasets.CIFAR10('./content/', download=True, train=False)\n",
        "\n",
        "# Convert the data to numpy arrays\n",
        "x_train_total = np.array(data_train.data)\n",
        "label_train_total = np.array(data_train.targets)\n",
        "\n",
        "split_index = 5000\n",
        "x_train_np = x_train_total[split_index:]\n",
        "label_train_np = label_train_total[split_index:]\n",
        "\n",
        "\n",
        "\n",
        "# Validation_Set\n",
        "x_validation_np = x_train_total[:split_index]\n",
        "label_validation_np = label_train_total[:split_index]\n",
        "\n",
        "\n",
        "\n",
        "x_test_np = np.array(data_test.data)\n",
        "label_test_np = np.array(data_test.targets)\n",
        "\n",
        "# Find an accelerator\n",
        "if torch.cuda.is_available():\n",
        "    my_device = torch.device(\"cuda\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    torch.backends.mps.is_built()\n",
        "    my_device = torch.device(\"mps\")\n",
        "else:\n",
        "    my_device = torch.device(\"cpu\")\n",
        "print(\"Running on: \",my_device)\n",
        "\n",
        "# Data normalization\n",
        "x_train_np = x_train_np / 255.0\n",
        "x_validation_np = x_validation_np / 255.0\n",
        "x_test_np = x_test_np / 255.0\n",
        "\n",
        "\n",
        "# Convert to tensors\n",
        "x_train     = torch.tensor(x_train_np, requires_grad=False, device=my_device, dtype=torch.float32)\n",
        "label_train = torch.tensor(label_train_np, requires_grad=False, device=my_device)\n",
        "x_validation     = torch.tensor(x_validation_np, requires_grad=False, device=my_device, dtype=torch.float32)\n",
        "label_validation = torch.tensor(label_validation_np, requires_grad=False, device=my_device)\n",
        "\n",
        "x_test     = torch.tensor(x_test_np, requires_grad=False, device=my_device, dtype=torch.float32)\n",
        "label_test = torch.tensor(label_test_np, requires_grad=False, device=my_device)\n",
        "\n",
        "#Set\n",
        "n_train = x_train.shape[0]\n",
        "n_test  = x_test.shape[0]\n",
        "n_validation = x_validation.shape[0]\n",
        "\n",
        "sY = x_train.shape[1]\n",
        "sX = x_train.shape[2]\n",
        "n_class = 10\n",
        "chan  = 3\n",
        "\n",
        "\n",
        "# Reshape the data\n",
        "x_train = torch.permute(x_train, (0,3,1,2))\n",
        "y_train = F.one_hot(label_train, n_class).type(torch.float32)\n",
        "\n",
        "x_validation = torch.permute(x_validation, (0,3,1,2))\n",
        "y_validation = F.one_hot(label_validation, n_class).type(torch.float32)\n",
        "\n",
        "x_test = torch.permute(x_test, (0,3,1,2))\n",
        "y_test = F.one_hot(label_test, n_class).type(torch.float32)\n",
        "\n",
        "# -----\n",
        "# Create Neural Network\n",
        "# -----\n",
        "class ResNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ResNet, self).__init__()\n",
        "    filters = 16\n",
        "    # start of first block\n",
        "    # nn.Conv2d(in_channels, out_channels, kernel_size, padding=1)\n",
        "    self.first_conv2d = nn.Conv2d(3,16,3,1,1)\n",
        "    self.bn1 = nn.BatchNorm2d(filters)\n",
        "    self.second_conv2d = nn.Conv2d(16,16,3,1,1)\n",
        "    self.bn2 = nn.BatchNorm2d(filters)\n",
        "\n",
        "    # start of second block\n",
        "    self.third_conv2d = nn.Conv2d(16,16,3,1,1)\n",
        "    self.bn3 = nn.BatchNorm2d(filters)\n",
        "    self.fourth_conv2d = nn.Conv2d(16,16,3,1,1)\n",
        "    self.bn4 = nn.BatchNorm2d(filters)\n",
        "\n",
        "    # start of third block\n",
        "    self.fifth_conv2d = nn.Conv2d(16,16,3,1,1)\n",
        "    self.bn5 = nn.BatchNorm2d(filters)\n",
        "    self.sixth_conv2d = nn.Conv2d(16,16,3,1,1)\n",
        "    self.bn6 = nn.BatchNorm2d(filters)\n",
        "\n",
        "    self.avg_pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "    self.first_linear = nn.Linear(1024, 512)\n",
        "    self.second_linear = nn.Linear(512, 10)\n",
        "\n",
        "  def forward(self,x):\n",
        "    batch_size = x.shape[0]\n",
        "    channels = x.shape[1]\n",
        "    rows = x.shape[2]\n",
        "    cols = x.shape[3]\n",
        "\n",
        "    # Start first block\n",
        "    a = self.first_conv2d(x)\n",
        "    a = self.bn1(a)\n",
        "    a = F.leaky_relu_(a)\n",
        "    a = self.second_conv2d(a)\n",
        "    a = self.bn2(a)\n",
        "    a[:, 0:3, :, :] = a[:, 0:3, :, :] + x # Skip Connection\n",
        "    a = F.leaky_relu_(a)\n",
        "    a = self.avg_pool(a)\n",
        "    # End first block\n",
        "\n",
        "    # Start second block\n",
        "    b = self.third_conv2d(a)\n",
        "    b = self.bn3(b)\n",
        "    b = F.leaky_relu_(b)\n",
        "    b = self.fourth_conv2d(b)\n",
        "    b = self.bn4(b)\n",
        "    b[:, 0:16, :, :] = b[:, 0:16, :, :] + a # Skip Connection\n",
        "    b = F.leaky_relu_(b)\n",
        "    b = self.avg_pool(b)\n",
        "    # End second block\n",
        "\n",
        "    # Start third block\n",
        "    c = self.fifth_conv2d(b)\n",
        "    c = self.bn5(c)\n",
        "    c = F.leaky_relu_(c)\n",
        "    c = self.sixth_conv2d(c)\n",
        "    c = self.bn6(c)\n",
        "    c[:, 0:16, :, :] = c[:, 0:16, :, :] + b # Skip Connection\n",
        "    d = F.leaky_relu_(c)\n",
        "    # End third block\n",
        "\n",
        "    # Reshape before last layers\n",
        "    d = d.view(batch_size, -1)\n",
        "\n",
        "    d = self.first_linear(d)\n",
        "    d = F.leaky_relu_(d)\n",
        "    d = self.second_linear(d)\n",
        "    d = F.softmax(d,dim=1)\n",
        "    return d\n",
        "\n",
        "# -----\n",
        "# Create model object\n",
        "# -----\n",
        "model = ResNet().to(my_device)\n",
        "\n",
        "# -----\n",
        "# Train the model\n",
        "# -----\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 100\n",
        "n_epoch = 100\n",
        "n_train = x_train.data.shape[0]\n",
        "n_batch_train = n_train // batch_size\n",
        "n_batch_validation = n_validation // batch_size\n",
        "n_batch_test = n_test // batch_size\n",
        "learningRate = 0.01\n",
        "prev_validation_loss = 0.0\n",
        "\n",
        "# create and optimizer\n",
        "optim = torch.optim.SGD(model.parameters(), lr = learningRate)\n",
        "\n",
        "train_loss_coords = []\n",
        "train_acc_coords = []\n",
        "\n",
        "validation_loss_coords = []\n",
        "validation_acc_coords = []\n",
        "\n",
        "test_loss_coords = []\n",
        "test_acc_coords = []\n",
        "\n",
        "\n",
        "for epoch in range(n_epoch):\n",
        "  print('-----')\n",
        "  train_epoch_loss = 0.0\n",
        "  train_total = 0\n",
        "  train_correct = 0\n",
        "\n",
        "  for batch in range(n_batch_train):\n",
        "    # Reset optimizer for gradient descent\n",
        "    optim.zero_grad()\n",
        "\n",
        "    # Start / end indicies of the data\n",
        "    sidx = batch * batch_size\n",
        "    eidx = (batch+1) * batch_size\n",
        "\n",
        "    # Grab the data and the labels for the batch\n",
        "    X = x_train[sidx:eidx]\n",
        "    Y = y_train[sidx:eidx]\n",
        "\n",
        "    # Run the model\n",
        "    Yhat = model(X)\n",
        "\n",
        "    # Loss\n",
        "    loss = -(torch.sum(Y*torch.log(Yhat+1e-15))/batch_size)\n",
        "\n",
        "    # Gradient descent\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "\n",
        "    # Keep track of the loss !!!\n",
        "    train_loss_np = loss.detach().cpu().numpy()\n",
        "    train_epoch_loss = train_epoch_loss + train_loss_np\n",
        "\n",
        "    # Accuracy\n",
        "    _, predicted = torch.max(Yhat.data,1)\n",
        "    _, Y_train = torch.max(Y,1)\n",
        "\n",
        "    train_total += Y_train.size(0)\n",
        "    train_correct += (predicted == Y_train).sum().item()\n",
        "\n",
        "  train_epoch_loss = train_epoch_loss / n_train\n",
        "  train_accuracy = (train_correct / (n_batch_train * batch_size)) * 100\n",
        "  print(f'Training: Epoch {epoch+1}/{n_epoch} {train_epoch_loss:.5f} Loss, {train_accuracy:.2f}% Accurate')\n",
        "  train_loss_coords.append(train_epoch_loss)\n",
        "  train_acc_coords.append(train_accuracy)\n",
        "\n",
        "  # Validation set\n",
        "  with torch.no_grad():\n",
        "    new_validation_loss = 0.0\n",
        "    validation_total = 0\n",
        "    validation_correct = 0\n",
        "    for batch in range(n_batch_validation):\n",
        "      # Start / end indicies of the data\n",
        "      sidx = batch * batch_size\n",
        "      eidx = (batch+1) * batch_size\n",
        "\n",
        "      # Grab the data and the labels for the batch\n",
        "      X = x_validation[sidx:eidx]\n",
        "      Y = y_validation[sidx:eidx]\n",
        "\n",
        "      # Run the model\n",
        "      Yhat = model(X)\n",
        "      # Loss\n",
        "      loss = -(torch.sum(Y*torch.log(Yhat+1e-15))/batch_size)\n",
        "\n",
        "      # Keep track of the loss !!!\n",
        "      validation_loss_np = loss.detach().cpu().numpy()\n",
        "      new_validation_loss = new_validation_loss + validation_loss_np\n",
        "\n",
        "      # Accuracy\n",
        "      _, predicted = torch.max(Yhat.data,1)\n",
        "      _, Y_validation = torch.max(Y,1)\n",
        "\n",
        "      validation_total += Y_validation.size(0)\n",
        "      validation_correct += (predicted == Y_validation).sum().item()\n",
        "\n",
        "\n",
        "    new_validation_loss = new_validation_loss / n_validation\n",
        "    validation_accuracy = (validation_correct / (n_batch_validation * batch_size)) * 100\n",
        "    print(f'Validation: Epoch {epoch+1}/{n_epoch} {new_validation_loss:.5f} Loss, {validation_accuracy:.2f}% Accurate')\n",
        "    validation_loss_coords.append(new_validation_loss)\n",
        "    validation_acc_coords.append(validation_accuracy)\n",
        "    if((new_validation_loss*.95) > prev_validation_loss) and (prev_validation_loss != 0):\n",
        "        learningRate = learningRate/10\n",
        "        print('Learning Rate Updated: ',learningRate)\n",
        "    prev_validation_loss = new_validation_loss\n",
        "\n",
        "\n",
        "# Test set\n",
        "test_loss = 0.0\n",
        "test_total = 0\n",
        "test_correct = 0\n",
        "images_pred = []\n",
        "images_label = y_test[:50]\n",
        "\n",
        "for batch in range(n_batch_test):\n",
        "      # Start / end indicies of the data\n",
        "      sidx = batch * batch_size\n",
        "      eidx = (batch+1) * batch_size\n",
        "\n",
        "      # Grab the data and the labels for the batch\n",
        "      X_test = x_test[sidx:eidx]\n",
        "      Y = y_test[sidx:eidx]\n",
        "\n",
        "      # Run the model\n",
        "      Yhat = model(X_test)\n",
        "      # Loss\n",
        "      loss = -(torch.sum(Y*torch.log(Yhat+1e-15))/batch_size)\n",
        "\n",
        "      # Keep track of the loss !!!\n",
        "      test_loss_np = loss.detach().cpu().numpy()\n",
        "      test_loss = test_loss + test_loss_np\n",
        "\n",
        "      # Accuracy\n",
        "      _, predicted = torch.max(Yhat.data,1)\n",
        "      _, Y_test = torch.max(Y,1)\n",
        "\n",
        "      test_total += Y_test.size(0)\n",
        "      test_correct += (predicted == Y_test).sum().item()\n",
        "\n",
        "      # Show the first 10 images from the training set\n",
        "      if(batch == 0):\n",
        "        images_pred = predicted\n",
        "\n",
        "\n",
        "test_loss = test_loss / n_test\n",
        "test_accuracy = (test_correct / (n_batch_test * batch_size)) * 100\n",
        "print('-----')\n",
        "print(f'Test: Epoch {epoch+1}/{n_epoch} Training: {test_loss:.5f} Loss, {test_accuracy:.2f}% Accurate')\n",
        "test_loss_coords.append(test_loss)\n",
        "test_acc_coords.append(test_accuracy)\n",
        "\n",
        "my_dict = dict([\n",
        "    (0, 'airplane'),\n",
        "    (1, 'automobile'),\n",
        "    (2, 'bird'),\n",
        "    (3, 'cat'),\n",
        "    (4, 'deer'),\n",
        "    (5, 'dog'),\n",
        "    (6, 'frog'),\n",
        "    (7, 'horse'),\n",
        "    (8, 'ship'),\n",
        "    (9, 'truck')])\n",
        "\n",
        "for x in range(50):\n",
        "  images_pred_np = images_pred[x].detach().cpu().numpy()\n",
        "  images_label_np = torch.argmax(images_label[x]).detach().cpu().numpy()\n",
        "  plt.figure()\n",
        "  plt.imshow(x_test_np[x])\n",
        "  # Add a title to the plot\n",
        "  plt.title(f'Test: {x}  Label:<{my_dict[int(images_label_np)]}>  Pred:<{my_dict[int(images_pred_np)]}>  Accuracy: {test_accuracy:.2f}')\n",
        "\n",
        "  # Save the figure as an image file (e.g., PNG)\n",
        "  filename = os.path.join(outdir, f'test_{x}.png')\n",
        "  plt.savefig(filename)\n",
        "  matplotlib.pyplot.close()\n",
        "\n",
        "# Create charts\n",
        "filename = os.path.join(outdir, 'loss.png')\n",
        "plt.savefig(filename)\n",
        "\n",
        "fig1 = plt.figure()\n",
        "plt.plot(list(range(len(train_loss_coords))), train_loss_coords, label='Train')\n",
        "plt.plot(list(range(len(validation_loss_coords))), validation_loss_coords, label='Validation')\n",
        "plt.axhline(y=test_loss_coords, label='Test', color='red')\n",
        "\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('ResNet-3 Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "filename = os.path.join(outdir, 'loss.png')\n",
        "plt.savefig(filename)\n",
        "\n",
        "fig2 = plt.figure()\n",
        "plt.plot(list(range(len(train_acc_coords))), train_acc_coords, label='Train')\n",
        "plt.plot(list(range(len(validation_acc_coords))), validation_acc_coords, label='Validation')\n",
        "plt.axhline(y=test_acc_coords, label='Test', color='red')\n",
        "\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('ResNet-3 Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "filename = os.path.join(outdir, 'accuracy.png')\n",
        "plt.savefig(filename)\n",
        "matplotlib.pyplot.close()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO5tTOaWC/Fc8o2LAaBrVcU",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}